# TPs Big Data avec Spark, Cassandra & Docker

## Auteur: **Mohamed YOULA** 

## Objectif 
Ces travaux pratiques visent à travailler sur les trois axes suivants: 
1. Manipulation de fichiers et **DataFrames** avec **Spark**    
2. Intégration de **Spark** avec **Cassandra** via **Docker**  
3. Classification non supervisée avec **K-Means** (MLlib)

---

## Contenu

### 1️⃣ TP Spark — Lecture & Écriture de DataFrames
- Lecture de fichiers **CSV**, **TXT**, **LIBSVM**
- Création de **DataFrames** depuis des **RDD**
- Inférence et définition de **schémas**
- Calcul de **statistiques descriptives**
- Écriture de DataFrames en **CSV**

---

### 2️⃣ TP Spark + Cassandra (Docker)
- Déploiement d’un conteneur **Cassandra** avec Docker
- Création d’une base et de tables (**Restaurant**, **Inspection**)
- Import de données réelles (restaurants NYC)
- Lecture depuis Spark avec le **Cassandra Connector**
- Traitements distribués : sélection, filtres, jointures, agrégations

---

### 3️⃣ TP Classification K-Means
- Génération de données 2D synthétiques
- Assemblage des variables en vecteurs de features
- Application de **K-Means** (k=5)
- Attribution des clusters aux points
- Évaluation par le **coefficient de silhouette**
- Affichage des **centres des clusters**

---

## Prérequis
- **Apache Spark 3.x**
- **Python (PySpark)** & **Scala**
- **Docker Desktop**
- **Cassandra** + **Spark Cassandra Connector**

---

## Connaissances acquises
- Savoir lire/écrire différents formats de données avec Spark  
- Connecter Spark à Cassandra et manipuler des données distribuées  
- Appliquer un algorithme de **clustering** (K-Means) et évaluer ses performances  

---
